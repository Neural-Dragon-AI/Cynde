{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this example, we focused on integrating Polars DataFrames with scikit-learn's pipeline and preprocessing functionality to create a streamlined and efficient machine learning workflow. The main objective was to leverage the power of Polars for data manipulation and the flexibility of scikit-learn for model training and evaluation.\n",
    "\n",
    "Here's a summary of what we accomplished:\n",
    "\n",
    "1. Data Preparation:\n",
    "   - We defined a `convert_utf8_to_enum` function to convert categorical columns in a Polars DataFrame from UTF-8 to the Enum data type based on a specified threshold.\n",
    "   - We created Pydantic classes (`Feature`, `NumericalFeature`, `EmbeddingFeature`, `CategoricalFeature`, `FeatureSet`, and `InputConfig`) to define and validate the feature sets used in the machine learning pipeline.\n",
    "   - We generated simulated data using the `generate_simulated_data` function to demonstrate the workflow.\n",
    "\n",
    "2. Pipeline Creation:\n",
    "   - We defined a `create_pipeline` function that takes an `InputConfig` instance and creates a scikit-learn `Pipeline` object.\n",
    "   - The pipeline consists of a `ColumnTransformer` for preprocessing numerical and categorical features, followed by a `LinearSVC` classifier.\n",
    "   - For numerical features, we used `StandardScaler` to standardize the data.\n",
    "   - For categorical features, we used `\"passthrough\"` to pass the physical representation of the Enum columns directly through the pipeline, avoiding the need for `OneHotEncoder`.\n",
    "   - We set the output of the pipeline to \"polars\" using `pipeline.set_output(transform=\"polars\")` to ensure that the pipeline returns a Polars DataFrame.\n",
    "\n",
    "3. Model Training and Evaluation:\n",
    "   - We simulated cross-validation by filtering the DataFrame based on a \"fold\" column to obtain the training, validation, and test sets.\n",
    "   - We fit the pipeline on the training data using `pipeline.fit()`.\n",
    "   - We evaluated the model's performance on the validation and test data using the `evaluate_model` function, which calculates the accuracy using scikit-learn's `accuracy_score`.\n",
    "\n",
    "4. Integration with Polars:\n",
    "   - Throughout the example, we used Polars DataFrames for data manipulation and preprocessing.\n",
    "   - We converted the categorical columns to their physical representation using `pl.col(col).to_physical()` to ensure compatibility with the pipeline.\n",
    "   - We used Polars' `filter` and `drop` methods to select the appropriate subsets of data for training, validation, and testing.\n",
    "\n",
    "In our next working session, we will focus on the following tasks:\n",
    "\n",
    "1. Refactoring the Cynde-related methods to integrate the new approaches developed in this example.\n",
    "2. Introducing equivalent models for defining the model configurations, replacing the current dictionary-based approach.\n",
    "3. Cleaning and generalizing all the cross-validation methods under a common framework to improve code organization and reusability.\n",
    "\n",
    "By building upon the foundation established in this example and incorporating the planned enhancements, we aim to create a more robust, efficient, and user-friendly machine learning framework within the Cynde project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from typing import List, Optional\n",
    "from pydantic import BaseModel, ValidationInfo, model_validator\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "def convert_utf8_to_enum(df: pl.DataFrame, threshold: float = 0.5) -> pl.DataFrame:\n",
    "    if not 0 < threshold < 1:\n",
    "        raise ValueError(\"Threshold must be between 0 and 1 (exclusive).\")\n",
    "\n",
    "    for column in df.columns:\n",
    "        if df[column].dtype == pl.Utf8 and len(df[column]) > 0:\n",
    "            unique_values = df[column].unique()\n",
    "            unique_ratio = len(unique_values) / len(df[column])\n",
    "\n",
    "            if unique_ratio <= threshold:\n",
    "                enum_dtype = pl.Enum(unique_values.to_list())\n",
    "                df = df.with_columns(df[column].cast(enum_dtype))\n",
    "            else:\n",
    "                print(f\"Column '{column}' has a high ratio of unique values ({unique_ratio:.2f}). Skipping conversion to Enum.\")\n",
    "        elif df[column].dtype == pl.Utf8 and len(df[column]) == 0:\n",
    "            print(f\"Column '{column}' is empty. Skipping conversion to Enum.\")\n",
    "\n",
    "    return df\n",
    "\n",
    "class Feature(BaseModel):\n",
    "    column_name: str\n",
    "    name: str\n",
    "    description: Optional[str] = None\n",
    "\n",
    "    @model_validator(mode='before')\n",
    "    def validate_column_name(cls, values):\n",
    "        column_name = values.get(\"column_name\")\n",
    "        context = values.get(\"context\")\n",
    "        if context is not None and isinstance(context, pl.DataFrame):\n",
    "            if column_name not in context.columns:\n",
    "                raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "        return values\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        extra = \"allow\"\n",
    "\n",
    "class NumericalFeature(Feature):\n",
    "    @model_validator(mode='before')\n",
    "    def validate_numerical_column(cls, values):\n",
    "        column_name = values.get(\"column_name\")\n",
    "        context = values.get(\"context\")\n",
    "        if context is not None and isinstance(context, pl.DataFrame):\n",
    "            if column_name not in context.columns:\n",
    "                raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "            if context[column_name].dtype not in [\n",
    "                pl.Boolean,\n",
    "                pl.Int8,\n",
    "                pl.Int16,\n",
    "                pl.Int32,\n",
    "                pl.Int64,\n",
    "                pl.UInt8,\n",
    "                pl.UInt16,\n",
    "                pl.UInt32,\n",
    "                pl.UInt64,\n",
    "                pl.Float32,\n",
    "                pl.Float64,\n",
    "                pl.Decimal,\n",
    "            ]:\n",
    "                raise ValueError(\n",
    "                    f\"Column '{column_name}' must be of a numeric type (Boolean, Integer, Unsigned Integer, Float, or Decimal).\"\n",
    "                )\n",
    "        return values\n",
    "\n",
    "class EmbeddingFeature(Feature):\n",
    "    @model_validator(mode='before')\n",
    "    def validate_embedding_column(cls, values):\n",
    "        column_name = values.get(\"column_name\")\n",
    "        context = values.get(\"context\")\n",
    "        if context is not None and isinstance(context, pl.DataFrame):\n",
    "            if column_name not in context.columns:\n",
    "                raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "            if context[column_name].dtype not in [pl.List(pl.Float32), pl.List(pl.Float64)]:\n",
    "                raise ValueError(f\"Column '{column_name}' must be of type pl.List(pl.Float32) or pl.List(pl.Float64).\")\n",
    "        return values\n",
    "\n",
    "class CategoricalFeature(Feature):\n",
    "    @model_validator(mode='before')\n",
    "    def validate_categorical_column(cls, values):\n",
    "        column_name = values.get(\"column_name\")\n",
    "        context = values.get(\"context\")\n",
    "        if context is not None and isinstance(context, pl.DataFrame):\n",
    "            if column_name not in context.columns:\n",
    "                raise ValueError(f\"Column '{column_name}' not found in the DataFrame.\")\n",
    "            if context[column_name].dtype not in [\n",
    "                pl.Utf8,\n",
    "                pl.Categorical,\n",
    "                pl.Enum,\n",
    "                pl.Int8,\n",
    "                pl.Int16,\n",
    "                pl.Int32,\n",
    "                pl.Int64,\n",
    "                pl.UInt8,\n",
    "                pl.UInt16,\n",
    "                pl.UInt32,\n",
    "                pl.UInt64,\n",
    "            ]:\n",
    "                raise ValueError(\n",
    "                    f\"Column '{column_name}' must be of type pl.Utf8, pl.Categorical, pl.Enum, or an integer type.\"\n",
    "                )\n",
    "        return values\n",
    "\n",
    "class FeatureSet(BaseModel):\n",
    "    numerical: List[NumericalFeature] = []\n",
    "    embeddings: List[EmbeddingFeature] = []\n",
    "    categorical: List[CategoricalFeature] = []\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        extra = \"allow\"\n",
    "\n",
    "class InputConfig(BaseModel):\n",
    "    feature_sets: List[FeatureSet]\n",
    "\n",
    "    def validate_with_dataframe(self, df: pl.DataFrame):\n",
    "        for feature_set in self.feature_sets:\n",
    "            for feature_type in [\"numerical\", \"embeddings\", \"categorical\"]:\n",
    "                for feature in getattr(feature_set, feature_type):\n",
    "                    feature.model_validate({\"context\": df, **feature.dict()})\n",
    "\n",
    "    class Config:\n",
    "        arbitrary_types_allowed = True\n",
    "        extra = \"allow\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_simulated_data(n_samples: int, n_classes: int) -> pl.DataFrame:\n",
    "    class_0 = np.random.multivariate_normal(mean=[30, 50000], cov=[[100, 0], [0, 1000000]], size=n_samples // 2)\n",
    "    class_1 = np.random.multivariate_normal(mean=[50, 80000], cov=[[100, 0], [0, 1000000]], size=n_samples // 2)\n",
    "    data = {\n",
    "        \"age\": np.concatenate((class_0[:, 0], class_1[:, 0])),\n",
    "        \"income\": np.concatenate((class_0[:, 1], class_1[:, 1])),\n",
    "        \"gender\": np.random.choice([\"Male\", \"Female\"], size=n_samples),\n",
    "        \"education\": np.random.choice([\"Bachelor's\", \"Master's\", \"PhD\"], size=n_samples),\n",
    "        \"target\": np.concatenate((np.zeros(n_samples // 2), np.ones(n_samples // 2))),\n",
    "        \"fold\": np.random.choice([0, 1, 2], size=n_samples),\n",
    "    }\n",
    "    return pl.DataFrame(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature set validation successful!\n",
      "Validation accuracy: 1.0\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(pipeline: Pipeline, X, y):\n",
    "    predictions = pipeline.predict(X)\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    return accuracy\n",
    "\n",
    "def create_pipeline(input_config: InputConfig) -> Pipeline:\n",
    "    transformers = []\n",
    "    for feature_set in input_config.feature_sets:\n",
    "        numerical_features = [feature.column_name for feature in feature_set.numerical]\n",
    "        if numerical_features:\n",
    "            transformers.append((\"numerical\", StandardScaler(), numerical_features))\n",
    "        categorical_features = [feature.column_name for feature in feature_set.categorical]\n",
    "        if categorical_features:\n",
    "            transformers.append((\"categorical\", \"passthrough\", categorical_features))\n",
    "    preprocessor = ColumnTransformer(transformers)\n",
    "    classifier = LinearSVC(dual=False)\n",
    "    pipeline = Pipeline([(\"preprocessor\", preprocessor), (\"classifier\", classifier)])\n",
    "    pipeline.set_output(transform=\"polars\")\n",
    "    return pipeline\n",
    "\n",
    "# Example usage\n",
    "n_samples = 1000\n",
    "n_classes = 2\n",
    "df = generate_simulated_data(n_samples, n_classes)\n",
    "\n",
    "# Convert categorical columns to Enum\n",
    "df_enum = convert_utf8_to_enum(df, threshold=0.8)\n",
    "\n",
    "# Convert Enum columns to their physical representation\n",
    "df_physical = df_enum.with_columns(\n",
    "    [pl.col(col).to_physical() for col in df_enum.columns if df_enum[col].dtype == pl.Enum]\n",
    ")\n",
    "\n",
    "# Declare feature sets using Pydantic classes\n",
    "numerical_features = [\n",
    "    NumericalFeature(column_name=\"age\", name=\"Age\"),\n",
    "    NumericalFeature(column_name=\"income\", name=\"Income\"),\n",
    "]\n",
    "categorical_features = [\n",
    "    CategoricalFeature(column_name=\"gender\", name=\"Gender\"),\n",
    "    CategoricalFeature(column_name=\"education\", name=\"Education\"),\n",
    "]\n",
    "feature_set = FeatureSet(\n",
    "    numerical=numerical_features,\n",
    "    categorical=categorical_features,\n",
    ")\n",
    "input_config = InputConfig(feature_sets=[feature_set])\n",
    "\n",
    "# Validate feature sets with the DataFrame\n",
    "try:\n",
    "    input_config.validate_with_dataframe(df_physical)\n",
    "    print(\"Feature set validation successful!\")\n",
    "except ValueError as e:\n",
    "    print(f\"Feature set validation failed: {str(e)}\")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = create_pipeline(input_config)\n",
    "\n",
    "# Simulate cross-validation\n",
    "fold_name = \"fold\"\n",
    "train_df = df_physical.filter(pl.col(fold_name) == 0)\n",
    "val_df = df_physical.filter(pl.col(fold_name) == 1)\n",
    "test_df = df_physical.filter(pl.col(fold_name) == 2)\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline.fit(train_df.drop(fold_name), train_df[\"target\"])\n",
    "\n",
    "# Evaluate the model on the validation and test data\n",
    "val_accuracy = evaluate_model(pipeline, val_df.drop([fold_name, \"target\"]), val_df[\"target\"])\n",
    "test_accuracy = evaluate_model(pipeline, test_df.drop([fold_name, \"target\"]), test_df[\"target\"])\n",
    "\n",
    "print(\"Validation accuracy:\", val_accuracy)\n",
    "print(\"Test accuracy:\", test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
