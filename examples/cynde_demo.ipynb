{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from typing import List\n",
    "from datetime import datetime\n",
    "import cynde.functional as cf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "api_key = os.getenv(\"OPENAI_API_KEY\")#\n",
    "client = openai.Client(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get current path with os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.getcwd()\n",
    "# Navigate one directory up to reach /cynde from /cynde/experiments\n",
    "parent_dir = os.path.dirname(script_dir)\n",
    "\n",
    "# Define the cache directory path as /cynde/cache\n",
    "cache_dir = os.path.join(parent_dir, \"cache\")\n",
    "\n",
    "# Ensure the cache directory exists, create if it doesn't\n",
    "os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "# Define file paths within the /cynde/cache directory\n",
    "requests_filepath = os.path.join(cache_dir, \"chat_payloads.jsonl\")\n",
    "results_filepath = os.path.join(cache_dir, \"openai_results.jsonl\")\n",
    "requests_filepath_ = os.path.join(cache_dir, \"chat_payloads_.jsonl\")\n",
    "results_filepath_ = os.path.join(cache_dir, \"openai_results_.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 4)\n",
      "┌─────────────┬───────────────────────────────────┬───────────┬─────────────────────┐\n",
      "│ customer_id ┆ feedback                          ┆ ratings   ┆ timestamp           │\n",
      "│ ---         ┆ ---                               ┆ ---       ┆ ---                 │\n",
      "│ i64         ┆ str                               ┆ list[i64] ┆ datetime[μs]        │\n",
      "╞═════════════╪═══════════════════════════════════╪═══════════╪═════════════════════╡\n",
      "│ 101         ┆ Loved the new product line!       ┆ [4, 5, 5] ┆ 2023-01-01 14:30:00 │\n",
      "│ 102         ┆ The service was disappointing th… ┆ [2, 3, 2] ┆ 2023-01-02 09:15:00 │\n",
      "│ 103         ┆ Great experience with customer s… ┆ [5, 4, 5] ┆ 2023-01-03 18:45:00 │\n",
      "└─────────────┴───────────────────────────────────┴───────────┴─────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "df = pl.DataFrame(\n",
    "    {\n",
    "        \"customer_id\": [101, 102, 103],\n",
    "        \"feedback\": [\n",
    "            \"Loved the new product line!\",\n",
    "            \"The service was disappointing this time.\",\n",
    "            \"Great experience with customer support.\",\n",
    "        ],\n",
    "        \"ratings\": [[4, 5, 5], [2, 3, 2], [5, 4, 5]],\n",
    "        \"timestamp\": [\n",
    "            datetime(2023, 1, 1, 14, 30),\n",
    "            datetime(2023, 1, 2, 9, 15),\n",
    "            datetime(2023, 1, 3, 18, 45),\n",
    "        ],\n",
    "    }\n",
    ")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating embeddings for column feedback\n",
      "Processing 3 chunks of text in a single batch\n",
      "Embedding Processing took 0.5426173210144043 seconds\n",
      "shape: (3, 5)\n",
      "┌─────────────┬─────────────────────────┬───────────┬─────────────────────┬────────────────────────┐\n",
      "│ customer_id ┆ feedback                ┆ ratings   ┆ timestamp           ┆ feedback_text-embeddin │\n",
      "│ ---         ┆ ---                     ┆ ---       ┆ ---                 ┆ g-3-small_…            │\n",
      "│ i64         ┆ str                     ┆ list[i64] ┆ datetime[μs]        ┆ ---                    │\n",
      "│             ┆                         ┆           ┆                     ┆ list[f64]              │\n",
      "╞═════════════╪═════════════════════════╪═══════════╪═════════════════════╪════════════════════════╡\n",
      "│ 101         ┆ Loved the new product   ┆ [4, 5, 5] ┆ 2023-01-01 14:30:00 ┆ [0.029205, -0.036287,  │\n",
      "│             ┆ line!                   ┆           ┆                     ┆ … 0.000765…            │\n",
      "│ 102         ┆ The service was         ┆ [2, 3, 2] ┆ 2023-01-02 09:15:00 ┆ [-0.005782, 0.019236,  │\n",
      "│             ┆ disappointing th…       ┆           ┆                     ┆ … -0.00427…            │\n",
      "│ 103         ┆ Great experience with   ┆ [5, 4, 5] ┆ 2023-01-03 18:45:00 ┆ [-0.014194, -0.027349, │\n",
      "│             ┆ customer s…             ┆           ┆                     ┆ … 0.02145…             │\n",
      "└─────────────┴─────────────────────────┴───────────┴─────────────────────┴────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "embedded_df = cf.embed_columns(df, [\"feedback\"], client=client)\n",
    "print(embedded_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 6)\n",
      "┌─────────────┬───────────────────┬───────────┬──────────────┬──────────────────┬──────────────────┐\n",
      "│ customer_id ┆ feedback          ┆ ratings   ┆ timestamp    ┆ feedback_text-em ┆ customer_prompt  │\n",
      "│ ---         ┆ ---               ┆ ---       ┆ ---          ┆ bedding-3-small_ ┆ ---              │\n",
      "│ i64         ┆ str               ┆ list[i64] ┆ datetime[μs] ┆ …                ┆ str              │\n",
      "│             ┆                   ┆           ┆              ┆ ---              ┆                  │\n",
      "│             ┆                   ┆           ┆              ┆ list[f64]        ┆                  │\n",
      "╞═════════════╪═══════════════════╪═══════════╪══════════════╪══════════════════╪══════════════════╡\n",
      "│ 101         ┆ Loved the new     ┆ [4, 5, 5] ┆ 2023-01-01   ┆ [0.029205,       ┆ Customer ID: 101 │\n",
      "│             ┆ product line!     ┆           ┆ 14:30:00     ┆ -0.036287, …     ┆ provided feedba… │\n",
      "│             ┆                   ┆           ┆              ┆ 0.000765…        ┆                  │\n",
      "│ 102         ┆ The service was   ┆ [2, 3, 2] ┆ 2023-01-02   ┆ [-0.005782,      ┆ Customer ID: 102 │\n",
      "│             ┆ disappointing th… ┆           ┆ 09:15:00     ┆ 0.019236, …      ┆ provided feedba… │\n",
      "│             ┆                   ┆           ┆              ┆ -0.00427…        ┆                  │\n",
      "│ 103         ┆ Great experience  ┆ [5, 4, 5] ┆ 2023-01-03   ┆ [-0.014194,      ┆ Customer ID: 103 │\n",
      "│             ┆ with customer s…  ┆           ┆ 18:45:00     ┆ -0.027349, …     ┆ provided feedba… │\n",
      "│             ┆                   ┆           ┆              ┆ 0.02145…         ┆                  │\n",
      "└─────────────┴───────────────────┴───────────┴──────────────┴──────────────────┴──────────────────┘\n",
      "Customer ID: 101 provided feedback at 14 with ratings 4-5-5 an average rating of 4.666666666666667 with a global mean of 3.8888888888888893: 'Loved the new product line!'\n",
      "Customer ID: 102 provided feedback at 9 with ratings 2-3-2 an average rating of 2.3333333333333335 with a global mean of 3.8888888888888893: 'The service was disappointing this time.'\n",
      "Customer ID: 103 provided feedback at 18 with ratings 5-4-5 an average rating of 4.666666666666667 with a global mean of 3.8888888888888893: 'Great experience with customer support.'\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Sample data frame initialization\n",
    "\n",
    "fstring = \"Customer ID: {} provided feedback at {} with ratings {} an average rating of {} with a global mean of {}: '{}'\"\n",
    "# Dynamic prompt generation with in-select computations\n",
    "\n",
    "df_prompted = cf.prompt(embedded_df, \n",
    "                     fstring,\n",
    "                     [pl.col(\"customer_id\"),\n",
    "                      pl.col(\"timestamp\").dt.hour(), #from timestamp to hour\n",
    "                      pl.col(\"ratings\").list.eval(pl.element().cast(pl.Utf8)).list.join(\"-\"), #needs to convert list columns to string\n",
    "                      pl.col(\"ratings\").list.mean(), #from list to float\n",
    "                      pl.col(\"ratings\").list.mean().mean(), #constant that gets broadcasted with pl.lit\n",
    "                      pl.col(\"feedback\")],\n",
    "                      \"customer_prompt\")\n",
    "print(df_prompted)\n",
    "for prompt in df_prompted[\"customer_prompt\"]:\n",
    "        print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cynde.functional.generate import generate_chat_completion_payloads, generate_chat_payloads_from_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"Evaluate the following customer feedback return a True or False based on the sentiment:\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 2)\n",
      "┌───────────────────────────────────┬───────────────────────────────────┐\n",
      "│ customer_prompt                   ┆ str_messages                      │\n",
      "│ ---                               ┆ ---                               │\n",
      "│ str                               ┆ str                               │\n",
      "╞═══════════════════════════════════╪═══════════════════════════════════╡\n",
      "│ Customer ID: 101 provided feedba… ┆ {\"role\":\"system\",\"content\":\"Eval… │\n",
      "│ Customer ID: 102 provided feedba… ┆ {\"role\":\"system\",\"content\":\"Eval… │\n",
      "│ Customer ID: 103 provided feedba… ┆ {\"role\":\"system\",\"content\":\"Eval… │\n",
      "└───────────────────────────────────┴───────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "payload_df = generate_chat_payloads_from_column(requests_filepath, df_prompted, \"customer_prompt\", system_prompt)\n",
    "print(payload_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import asyncio\n",
    "from cynde.async_tools.api_request_parallel_processor import process_api_requests_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cynde.functional.generate import process_and_merge_llm_responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Starting request #0\n",
      "INFO:root:Starting request #1\n",
      "INFO:root:Starting request #2\n",
      "INFO:root:Parallel processing complete. Results saved to c:\\Users\\Tommaso\\Documents\\Dev\\Cynde\\cache\\openai_results.jsonl\n"
     ]
    }
   ],
   "source": [
    "request_url = \"https://api.openai.com/v1/chat/completions\"  # Replace with your actual API endpoint\n",
    "    # Process multiple api requests to ChatGPT\n",
    "asyncio.run(\n",
    "    process_api_requests_from_file(\n",
    "        requests_filepath=requests_filepath,\n",
    "        save_filepath=results_filepath,\n",
    "        request_url=request_url,\n",
    "        api_key=api_key,\n",
    "        max_requests_per_minute=float(90000),\n",
    "        max_tokens_per_minute=float(170000),\n",
    "        token_encoding_name=\"cl100k_base\",\n",
    "        max_attempts=int(5),\n",
    "        logging_level=int(20),\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cynde.functional.generate import merge_df_with_openai_results,load_openai_results_jsonl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 4)\n",
      "┌──────────────────────────┬───────────────────────┬───────────┬───────────────────────────────────┐\n",
      "│ messages                 ┆ choices               ┆ usage     ┆ str_messages                      │\n",
      "│ ---                      ┆ ---                   ┆ ---       ┆ ---                               │\n",
      "│ list[struct[2]]          ┆ struct[2]             ┆ struct[3] ┆ str                               │\n",
      "╞══════════════════════════╪═══════════════════════╪═══════════╪═══════════════════════════════════╡\n",
      "│ [{\"system\",\"Evaluate the ┆ {\"assistant\",\"True\"}  ┆ {79,1,80} ┆ {\"role\":\"system\",\"content\":\"Eval… │\n",
      "│ followi…                 ┆                       ┆           ┆                                   │\n",
      "│ [{\"system\",\"Evaluate the ┆ {\"assistant\",\"False\"} ┆ {80,1,81} ┆ {\"role\":\"system\",\"content\":\"Eval… │\n",
      "│ followi…                 ┆                       ┆           ┆                                   │\n",
      "│ [{\"system\",\"Evaluate the ┆ {\"assistant\",\"True\"}  ┆ {78,1,79} ┆ {\"role\":\"system\",\"content\":\"Eval… │\n",
      "│ followi…                 ┆                       ┆           ┆                                   │\n",
      "└──────────────────────────┴───────────────────────┴───────────┴───────────────────────────────────┘\n"
     ]
    }
   ],
   "source": [
    "results_df = load_openai_results_jsonl(results_filepath)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape: (3, 9)\n",
      "┌───────────┬───────────┬───────────┬───────────┬───┬───────────┬───────────┬───────────┬──────────┐\n",
      "│ customer_ ┆ feedback  ┆ ratings   ┆ timestamp ┆ … ┆ customer_ ┆ messages  ┆ choices   ┆ usage    │\n",
      "│ id        ┆ ---       ┆ ---       ┆ ---       ┆   ┆ prompt    ┆ ---       ┆ ---       ┆ ---      │\n",
      "│ ---       ┆ str       ┆ list[i64] ┆ datetime[ ┆   ┆ ---       ┆ list[stru ┆ struct[2] ┆ struct[3 │\n",
      "│ i64       ┆           ┆           ┆ μs]       ┆   ┆ str       ┆ ct[2]]    ┆           ┆ ]        │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════════╪═══╪═══════════╪═══════════╪═══════════╪══════════╡\n",
      "│ 101       ┆ Loved the ┆ [4, 5, 5] ┆ 2023-01-0 ┆ … ┆ Customer  ┆ [{\"system ┆ {\"assista ┆ {79,1,80 │\n",
      "│           ┆ new       ┆           ┆ 1         ┆   ┆ ID: 101   ┆ \",\"Evalua ┆ nt\",\"True ┆ }        │\n",
      "│           ┆ product   ┆           ┆ 14:30:00  ┆   ┆ provided  ┆ te the    ┆ \"}        ┆          │\n",
      "│           ┆ line!     ┆           ┆           ┆   ┆ feedba…   ┆ followi…  ┆           ┆          │\n",
      "│ 102       ┆ The       ┆ [2, 3, 2] ┆ 2023-01-0 ┆ … ┆ Customer  ┆ [{\"system ┆ {\"assista ┆ {80,1,81 │\n",
      "│           ┆ service   ┆           ┆ 2         ┆   ┆ ID: 102   ┆ \",\"Evalua ┆ nt\",\"Fals ┆ }        │\n",
      "│           ┆ was disap ┆           ┆ 09:15:00  ┆   ┆ provided  ┆ te the    ┆ e\"}       ┆          │\n",
      "│           ┆ pointing  ┆           ┆           ┆   ┆ feedba…   ┆ followi…  ┆           ┆          │\n",
      "│           ┆ th…       ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "│ 103       ┆ Great exp ┆ [5, 4, 5] ┆ 2023-01-0 ┆ … ┆ Customer  ┆ [{\"system ┆ {\"assista ┆ {78,1,79 │\n",
      "│           ┆ erience   ┆           ┆ 3         ┆   ┆ ID: 103   ┆ \",\"Evalua ┆ nt\",\"True ┆ }        │\n",
      "│           ┆ with      ┆           ┆ 18:45:00  ┆   ┆ provided  ┆ te the    ┆ \"}        ┆          │\n",
      "│           ┆ customer  ┆           ┆           ┆   ┆ feedba…   ┆ followi…  ┆           ┆          │\n",
      "│           ┆ s…        ┆           ┆           ┆   ┆           ┆           ┆           ┆          │\n",
      "└───────────┴───────────┴───────────┴───────────┴───┴───────────┴───────────┴───────────┴──────────┘\n"
     ]
    }
   ],
   "source": [
    "merged_df = merge_df_with_openai_results(df_prompted, payload_df, results_df, \"customer_prompt\")\n",
    "print(merged_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating chat completion payloads...\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: '_c:\\\\Users\\\\Tommaso\\\\Documents\\\\Dev\\\\Cynde\\\\cache\\\\chat_payloads.jsonl'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m merged_df \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_and_merge_llm_responses\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdf_prompted\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcustomer_prompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                                \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mrequests_filepath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mrequests_filepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mresults_filepath\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43mresults_filepath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                                \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(merged_df)\n",
      "File \u001b[1;32mc:\\users\\tommaso\\documents\\dev\\cynde\\cynde\\functional\\generate.py:89\u001b[0m, in \u001b[0;36mprocess_and_merge_llm_responses\u001b[1;34m(df, column_name, system_prompt, requests_filepath, results_filepath, api_key, model_name)\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerating chat completion payloads...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     88\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m---> 89\u001b[0m payload_df \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_chat_payloads_from_column\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequests_filepath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_prompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     90\u001b[0m end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat completion payloads generated in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\tommaso\\documents\\dev\\cynde\\cynde\\functional\\generate.py:27\u001b[0m, in \u001b[0;36mgenerate_chat_payloads_from_column\u001b[1;34m(filename, df, column_name, prompt, model_name)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Uses the hacky loop to generate chat payloads from a column in a DataFrame. Has to be rewritten using the Polars Rust backend.\"\"\"\u001b[39;00m\n\u001b[0;32m     26\u001b[0m data \u001b[38;5;241m=\u001b[39m df[column_name]\u001b[38;5;241m.\u001b[39mto_list()\n\u001b[1;32m---> 27\u001b[0m \u001b[43mgenerate_chat_completion_payloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m#return a dataframe with the generated payloads and the original column name\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;66;03m#load the generated payloads\u001b[39;00m\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m pl\u001b[38;5;241m.\u001b[39mconcat([df\u001b[38;5;241m.\u001b[39mselect(pl\u001b[38;5;241m.\u001b[39mcol(column_name)), pl\u001b[38;5;241m.\u001b[39mread_ndjson(filename)], how \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhorizontal\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mselect(pl\u001b[38;5;241m.\u001b[39mcol(column_name),list_struct_to_string(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\users\\tommaso\\documents\\dev\\cynde\\cynde\\functional\\generate.py:11\u001b[0m, in \u001b[0;36mgenerate_chat_completion_payloads\u001b[1;34m(filename, data, prompt, model_name)\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_chat_completion_payloads\u001b[39m(filename:\u001b[38;5;28mstr\u001b[39m, data:\u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m], prompt:\u001b[38;5;28mstr\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-3.5-turbo\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Hacky in Python with a loop, but it works.\"\"\"\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m     12\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m data:\n\u001b[0;32m     13\u001b[0m             \u001b[38;5;66;03m# Create a list of messages for each request\u001b[39;00m\n\u001b[0;32m     14\u001b[0m             messages \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     15\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompt},\n\u001b[0;32m     16\u001b[0m                 {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mstr\u001b[39m(x)}\n\u001b[0;32m     17\u001b[0m             ]\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: '_c:\\\\Users\\\\Tommaso\\\\Documents\\\\Dev\\\\Cynde\\\\cache\\\\chat_payloads.jsonl'"
     ]
    }
   ],
   "source": [
    "merged_df = process_and_merge_llm_responses(df= df_prompted,\n",
    "                                column_name= \"customer_prompt\",\n",
    "                                system_prompt = system_prompt,\n",
    "                                requests_filepath = requests_filepath_,\n",
    "                                results_filepath = results_filepath_,\n",
    "                                api_key=api_key,)\n",
    "print(merged_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
